---
layout: distill
title: Lecture 02: Bayesian Networks
description: TBD
date: 2019-01-16

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Cathy Su
    url: "https://ceesu.github.io"   
  - name: Author 2
    url: "#"
  - name: Author 3
    url: "#"

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---


## Koller and Friedman Textbook Ch 3: The Bayesian network representation

### Exploiting independence Properties
#### Standard vs compact parametrization of  independent random variables
Given random variables $$X_i$$ each representing the outcome of an independent coin toss, the standard parametrization of their joint distribution would be as follows:
$$ P(X_1, ... X_n) = P(X_1 = x_1, X_2=x_2, ..., X_n = x_n) = P(X_1 = x_1)P(X_2=x_2)...P(X_n=x_n)$$
Note there are 2 possibilities for each outcome $$x_i$$, this representation requires $$2^n$$ total number of parameters. 
One simple way to reduce the number of parameters needed, would be to represent the probability that each coin toss lands heads as the n parameters $$\theta_1, ... \theta_n$$.  Then we have the following compact representation requiring only $$n$$ parameters:
$$ P(X_1, ... X_n) = \prod_i \theta_{X_i}$$

## Naive Bayes
We can further express the joint distribution in terms of conditional probabilities. This may be even more compact than the above if we use the 'naive' conditional independence assumption:
$$P(x_i | x_1, \dots, x_n) = P(x_i | y),$$


***

## Citations

